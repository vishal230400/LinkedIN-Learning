{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert named param 199\n",
      "Embedding Layer\n",
      "embeddings.word_embeddings.weight (30522, 768)\n",
      "embeddings.position_embeddings.weight (512, 768)\n",
      "embeddings.token_type_embeddings.weight (2, 768)\n",
      "embeddings.LayerNorm.weight (768,)\n",
      "embeddings.LayerNorm.bias (768,)\n",
      "First Encoder\n",
      "encoder.layer.0.attention.self.query.weight (768, 768)\n",
      "encoder.layer.0.attention.self.query.bias (768,)\n",
      "encoder.layer.0.attention.self.key.weight (768, 768)\n",
      "encoder.layer.0.attention.self.key.bias (768,)\n",
      "encoder.layer.0.attention.self.value.weight (768, 768)\n",
      "encoder.layer.0.attention.self.value.bias (768,)\n",
      "encoder.layer.0.attention.output.dense.weight (768, 768)\n",
      "encoder.layer.0.attention.output.dense.bias (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.weight (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.bias (768,)\n",
      "encoder.layer.0.intermediate.dense.weight (3072, 768)\n",
      "encoder.layer.0.intermediate.dense.bias (3072,)\n",
      "encoder.layer.0.output.dense.weight (768, 3072)\n",
      "encoder.layer.0.output.dense.bias (768,)\n",
      "encoder.layer.0.output.LayerNorm.weight (768,)\n",
      "encoder.layer.0.output.LayerNorm.bias (768,)\n",
      "Output Layer\n",
      "pooler.dense.weight (768, 768)\n",
      "pooler.dense.bias (768,)\n"
     ]
    }
   ],
   "source": [
    "named_params=list(model.named_parameters())\n",
    "print(\"Bert named param \"+str(len(named_params)))\n",
    "\n",
    "print(\"Embedding Layer\")\n",
    "for p in named_params[0:5]:\n",
    "    print(p[0]+\" \"+str(tuple(p[1].size())))\n",
    "\n",
    "print(\"First Encoder\")\n",
    "for p in named_params[5:21]:\n",
    "    print(p[0]+\" \"+str(tuple(p[1].size())))\n",
    "\n",
    "print(\"Output Layer\")\n",
    "for p in named_params[-2:]:\n",
    "    print(p[0]+\" \"+str(tuple(p[1].size())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8254, 2319, 7459, 1037, 3376, 2154, 102]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Sinan loves a beautiful day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=model(torch.tensor(tokenizer.encode('Sinan loves a beautiful day')).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.2327,  0.1515, -0.0448,  ..., -0.5192,  0.4195,  0.2948],\n",
       "         [ 0.3051, -0.6614,  0.2500,  ..., -0.9809,  0.2551,  0.2400],\n",
       "         [-0.3610, -0.8759,  0.4542,  ..., -1.1120,  0.1791,  0.0664],\n",
       "         ...,\n",
       "         [ 0.0689, -0.0364,  0.4940,  ..., -0.6558,  0.2227, -0.3868],\n",
       "         [-0.2657, -0.4257,  0.0056,  ...,  0.1352,  0.3596, -0.4585],\n",
       "         [ 0.6100,  0.0263, -0.2532,  ..., -0.0680, -0.3901, -0.3541]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8777, -0.4542, -0.6287,  0.7511,  0.3151, -0.0913,  0.9175,  0.3766,\n",
       "         -0.3059, -1.0000, -0.0577,  0.7535,  0.9913,  0.2113,  0.9418, -0.5328,\n",
       "         -0.0568, -0.5698,  0.4090, -0.6096,  0.7876,  0.9995,  0.3670,  0.2453,\n",
       "          0.4620,  0.9465, -0.6802,  0.9342,  0.9614,  0.7060, -0.5755,  0.2076,\n",
       "         -0.9910, -0.1697, -0.8019, -0.9952,  0.3786, -0.7309, -0.0599, -0.0186,\n",
       "         -0.8722,  0.3377,  0.9999, -0.0416,  0.3039, -0.2458, -1.0000,  0.2803,\n",
       "         -0.8856,  0.5071,  0.5182,  0.3099,  0.0669,  0.5149,  0.4535,  0.2464,\n",
       "         -0.0333,  0.1802, -0.2397, -0.5966, -0.5891,  0.4319, -0.6395, -0.9313,\n",
       "          0.4686,  0.1766, -0.1505, -0.2957, -0.0514, -0.1576,  0.8782,  0.2825,\n",
       "          0.1316, -0.8860,  0.1212,  0.2367, -0.4869,  1.0000, -0.3583, -0.9821,\n",
       "          0.5884,  0.2668,  0.4513,  0.3816, -0.0514, -1.0000,  0.3832, -0.1374,\n",
       "         -0.9907,  0.1855,  0.6004, -0.1374,  0.3874,  0.4813, -0.3741, -0.4017,\n",
       "         -0.3040, -0.5284, -0.3583, -0.2307,  0.0355, -0.3318, -0.2008, -0.3483,\n",
       "          0.2099, -0.4482, -0.6539,  0.3030, -0.2450,  0.7254,  0.2816, -0.3301,\n",
       "          0.4079, -0.9599,  0.6782, -0.3683, -0.9862, -0.4576, -0.9919,  0.7549,\n",
       "         -0.2049, -0.2972,  0.9678, -0.0692,  0.2751, -0.1389, -0.5539, -1.0000,\n",
       "         -0.5388, -0.3930,  0.1323, -0.2860, -0.9817, -0.9473,  0.6096,  0.9473,\n",
       "          0.1899,  0.9998, -0.4162,  0.9608,  0.0330, -0.2275,  0.3574, -0.4356,\n",
       "          0.6688,  0.2626, -0.6718,  0.1865, -0.1303,  0.4140, -0.4800, -0.3361,\n",
       "         -0.3105, -0.9495, -0.3550,  0.9570, -0.3836, -0.4769,  0.5362, -0.2674,\n",
       "         -0.5366,  0.8518,  0.3751,  0.4420, -0.1930,  0.4464,  0.1023,  0.5376,\n",
       "         -0.8681,  0.1531,  0.3970, -0.2881, -0.5797, -0.9841, -0.3196,  0.4932,\n",
       "          0.9899,  0.8004,  0.2937,  0.5190, -0.2158,  0.4647, -0.9605,  0.9846,\n",
       "         -0.2993,  0.3152, -0.3622,  0.2491, -0.8298, -0.0492,  0.8649, -0.5621,\n",
       "         -0.8109,  0.0596, -0.4256, -0.4160, -0.6402,  0.4875, -0.3528, -0.4344,\n",
       "         -0.1472,  0.9358,  0.9607,  0.8050, -0.2240,  0.6120, -0.9213, -0.3720,\n",
       "          0.1944,  0.2549,  0.2056,  0.9949, -0.3452, -0.1319, -0.9423, -0.9890,\n",
       "         -0.0635, -0.8755, -0.0717, -0.6739,  0.5520, -0.1817,  0.0886,  0.3924,\n",
       "         -0.9839, -0.8045,  0.4063, -0.3791,  0.5224, -0.2002,  0.7146,  0.8403,\n",
       "         -0.5794,  0.6525,  0.8934, -0.6888, -0.7708,  0.7749, -0.2771,  0.8685,\n",
       "         -0.7006,  0.9929,  0.6825,  0.5335, -0.9562, -0.3845, -0.8773, -0.4100,\n",
       "         -0.1682, -0.2743,  0.5831,  0.4320,  0.3957,  0.7450, -0.6908,  0.9976,\n",
       "         -0.8594, -0.9585, -0.6457, -0.2748, -0.9892,  0.7752,  0.3780,  0.1494,\n",
       "         -0.4580, -0.6380, -0.9619,  0.8854,  0.1650,  0.9833, -0.3250, -0.9306,\n",
       "         -0.5682, -0.9348, -0.2212, -0.1823,  0.2422, -0.1038, -0.9702,  0.5281,\n",
       "          0.5707,  0.5068, -0.3112,  0.9983,  1.0000,  0.9805,  0.8844,  0.8857,\n",
       "         -0.9983, -0.6119,  1.0000, -0.9533, -1.0000, -0.9239, -0.6746,  0.2397,\n",
       "         -1.0000, -0.1235, -0.0725, -0.9380,  0.1567,  0.9808,  0.9917, -1.0000,\n",
       "          0.9008,  0.9440, -0.5201,  0.8251, -0.4824,  0.9775,  0.4022,  0.5217,\n",
       "         -0.2950,  0.4242, -0.7700, -0.8768, -0.1279, -0.4046,  0.9811,  0.1531,\n",
       "         -0.7156, -0.9465,  0.4008, -0.1710, -0.0315, -0.9656, -0.2111,  0.4102,\n",
       "          0.7231,  0.1516,  0.3509, -0.8021,  0.2328, -0.5455,  0.4320,  0.5908,\n",
       "         -0.9447, -0.6176,  0.2716, -0.3587, -0.2995, -0.9437,  0.9684, -0.4916,\n",
       "          0.6288,  1.0000,  0.4189, -0.8921,  0.5297,  0.2667, -0.2394,  1.0000,\n",
       "          0.7230, -0.9832, -0.4890,  0.6689, -0.5466, -0.4932,  0.9996, -0.2217,\n",
       "         -0.1167,  0.0490,  0.9794, -0.9923,  0.9471, -0.9063, -0.9776,  0.9691,\n",
       "          0.9429, -0.4869, -0.7447,  0.1666, -0.3527,  0.3335, -0.9478,  0.7168,\n",
       "          0.3545, -0.1353,  0.9109, -0.7881, -0.4324,  0.3767, -0.1721,  0.1382,\n",
       "          0.8000,  0.5948, -0.2708,  0.0650, -0.3323, -0.4880, -0.9694,  0.2312,\n",
       "          1.0000, -0.2423,  0.4368, -0.3108, -0.0890, -0.1536,  0.4793,  0.4995,\n",
       "         -0.3567, -0.8845,  0.5553, -0.9447, -0.9911,  0.6794,  0.2624, -0.2930,\n",
       "          1.0000,  0.4425,  0.2925,  0.1576,  0.8787, -0.0432,  0.4729,  0.3847,\n",
       "          0.9754, -0.2582,  0.4169,  0.8205, -0.4750, -0.2669, -0.7019,  0.0698,\n",
       "         -0.9207,  0.1223, -0.9688,  0.9624,  0.6146,  0.3767,  0.1965,  0.4386,\n",
       "          1.0000, -0.5764,  0.5277, -0.1075,  0.8040, -0.9955, -0.7789, -0.4554,\n",
       "         -0.0230, -0.3707, -0.2666,  0.2837, -0.9743,  0.1491,  0.5257, -0.9831,\n",
       "         -0.9945,  0.0898,  0.7444,  0.1715, -0.9190, -0.7186, -0.5349,  0.5420,\n",
       "         -0.2393, -0.9546,  0.3902, -0.2898,  0.4637, -0.2010,  0.4773,  0.2017,\n",
       "          0.7901, -0.2781, -0.0836, -0.1398, -0.7673,  0.7647, -0.7953, -0.7866,\n",
       "         -0.0740,  1.0000, -0.5140,  0.5606,  0.7284,  0.6431, -0.1287,  0.1687,\n",
       "          0.8059,  0.3201, -0.3073, -0.1484, -0.5094, -0.4046,  0.4966,  0.2570,\n",
       "          0.2268,  0.8138,  0.6051,  0.1701,  0.0969,  0.0365,  0.9994, -0.2530,\n",
       "         -0.3062, -0.4775, -0.0299, -0.3373, -0.0562,  1.0000,  0.3479,  0.3388,\n",
       "         -0.9930, -0.6483, -0.9312,  1.0000,  0.7832, -0.8142,  0.6100,  0.6160,\n",
       "         -0.0965,  0.7704, -0.2299, -0.2138,  0.3634,  0.1423,  0.9668, -0.4813,\n",
       "         -0.9708, -0.5214,  0.4172, -0.9622,  0.9989, -0.5598, -0.2905, -0.4267,\n",
       "         -0.4151,  0.2529,  0.1150, -0.9828, -0.2933,  0.1020,  0.9736,  0.2178,\n",
       "         -0.4881, -0.8858,  0.4018,  0.1948, -0.6173, -0.9628,  0.9806, -0.9815,\n",
       "          0.6556,  1.0000,  0.2375, -0.3275,  0.1889, -0.4338,  0.3769, -0.5125,\n",
       "          0.5513, -0.9591, -0.3053, -0.2294,  0.3810, -0.1582, -0.5999,  0.7795,\n",
       "          0.1466, -0.4206, -0.6070, -0.0838,  0.4119,  0.8152, -0.3367, -0.1378,\n",
       "          0.0598, -0.0126, -0.9555, -0.4236, -0.3401, -0.9998,  0.5610, -1.0000,\n",
       "          0.2936, -0.2596, -0.2057,  0.8384,  0.5396,  0.3741, -0.7824, -0.2065,\n",
       "          0.7995,  0.7801, -0.2754, -0.2633, -0.7891,  0.3725, -0.0966,  0.4469,\n",
       "         -0.2534,  0.7190, -0.2084,  1.0000,  0.0228, -0.5043, -0.9706,  0.2989,\n",
       "         -0.3006,  1.0000, -0.8886, -0.9492,  0.5443, -0.5613, -0.8245,  0.3681,\n",
       "         -0.0980, -0.7511, -0.7760,  0.9546,  0.7718, -0.4745,  0.5655, -0.2965,\n",
       "         -0.4881, -0.0135,  0.6650,  0.9903,  0.5524,  0.8905, -0.2984, -0.1938,\n",
       "          0.9551,  0.2327,  0.6217,  0.2077,  1.0000,  0.3327, -0.9078,  0.1122,\n",
       "         -0.9812, -0.1818, -0.9299,  0.3491,  0.2149,  0.8971, -0.2581,  0.9697,\n",
       "         -0.4896,  0.0960, -0.3364,  0.1812,  0.4807, -0.9276, -0.9868, -0.9900,\n",
       "          0.5835, -0.4884,  0.0305,  0.3250,  0.0817,  0.4666,  0.5004, -1.0000,\n",
       "          0.9463,  0.4286,  0.5428,  0.9733,  0.5760,  0.5608,  0.2607, -0.9877,\n",
       "         -0.9753, -0.4241, -0.2934,  0.8090,  0.7190,  0.8551,  0.4776, -0.4918,\n",
       "         -0.4156, -0.0276, -0.8160, -0.9934,  0.3963,  0.0233, -0.9409,  0.9629,\n",
       "         -0.4148, -0.1184,  0.3304, -0.5221,  0.9129,  0.8248,  0.4409,  0.2048,\n",
       "          0.4997,  0.9182,  0.9484,  0.9918, -0.5874,  0.7747, -0.3111,  0.5266,\n",
       "          0.6146, -0.9457,  0.1223,  0.1749, -0.2750,  0.3174, -0.2500, -0.9335,\n",
       "          0.7000, -0.2273,  0.5874, -0.4977,  0.0328, -0.4565, -0.1727, -0.7292,\n",
       "         -0.5687,  0.5303,  0.2038,  0.8994,  0.8242, -0.0432, -0.6686, -0.3017,\n",
       "         -0.2925, -0.9236,  0.9091, -0.1265,  0.0754,  0.1207,  0.0477,  0.8970,\n",
       "         -0.0236, -0.4224, -0.3772, -0.7444,  0.9107, -0.3409, -0.5596, -0.5428,\n",
       "          0.8507,  0.3878,  0.9998, -0.4650, -0.5662, -0.3292, -0.3968,  0.3775,\n",
       "         -0.5127, -1.0000,  0.4716, -0.1930,  0.5168, -0.3026,  0.5821, -0.3216,\n",
       "         -0.9742, -0.3125,  0.2638,  0.1447, -0.5504, -0.5584,  0.4720, -0.1662,\n",
       "          0.9081,  0.9046,  0.2194,  0.5707,  0.5176, -0.1037, -0.7264,  0.9025]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2327,  0.1515, -0.0448,  ..., -0.5192,  0.4195,  0.2948],\n",
       "         [ 0.3051, -0.6614,  0.2500,  ..., -0.9809,  0.2551,  0.2400],\n",
       "         [-0.3610, -0.8759,  0.4542,  ..., -1.1120,  0.1791,  0.0664],\n",
       "         ...,\n",
       "         [ 0.0689, -0.0364,  0.4940,  ..., -0.6558,  0.2227, -0.3868],\n",
       "         [-0.2657, -0.4257,  0.0056,  ...,  0.1352,  0.3596, -0.4585],\n",
       "         [ 0.6100,  0.0263, -0.2532,  ..., -0.0680, -0.3901, -0.3541]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLS_embedding=response.last_hidden_state[:,0,:].unsqueeze(0)\n",
    "CLS_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler(CLS_embedding).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.pooler(CLS_embedding)==response.pooler_output).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total param : 109360128\n"
     ]
    }
   ],
   "source": [
    "total_param=0\n",
    "for p in model.parameters():\n",
    "    if len(p.shape)==2:\n",
    "        total_param+=p.shape[0]*p.shape[1]\n",
    "print(\"Total param : \"+str(total_param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
